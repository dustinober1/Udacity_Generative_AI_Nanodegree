{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Hugging Face Tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll explore Hugging Face's tokenizers by using a pretrained\n",
    "model. Hugging Face has many tokenizers available that have already been trained\n",
    "for specific models and tasks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 49.0/49.0 [00:00<00:00, 247kB/s]\n"
     ]
    }
   ],
   "source": [
    "# Choose a pretrained tokenizer to use\n",
    "my_tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding: Text to Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens: String Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Harry', 'Potter', 'and', 'the', 'So', '##rcerer', \"'\", 's', 'Stone', '(', 'chapter', '1', ')', 'CHAPTER', 'ONE', 'THE', 'B', '##O', '##Y', 'WHO', 'L', '##IVE', '##D', 'Mr', '.', 'and', 'Mrs', '.', 'Du', '##rs', '##ley', ',', 'of', 'number', 'four', ',', 'P', '##rive', '##t', 'Drive', ',', 'were', 'proud', 'to', 'say', 'that', 'they', 'were', 'perfectly', 'normal', ',', 'thank', 'you', 'very', 'much', '.', 'They', 'were', 'the', 'last', 'people', 'you', \"'\", 'd', 'expect', 'to', 'be', 'involved', 'in', 'anything', 'strange', 'or', 'mysterious', ',', 'because', 'they', 'just', 'didn', \"'\", 't', 'hold', 'with', 'such', 'nonsense', '.', 'Mr', '.', 'Du', '##rs', '##ley', 'was', 'the', 'director', 'of', 'a', 'firm', 'called', 'G', '##run', '##ning', '##s', ',', 'which', 'made', 'drill', '##s', '.', 'He', 'was', 'a', 'big', ',', 'beef', '##y', 'man', 'with', 'hardly', 'any', 'neck', ',', 'although', 'he', 'did', 'have', 'a', 'very', 'large', 'must', '##ache', '.', 'Mrs', '.', 'Du', '##rs', '##ley', 'was', 'thin', 'and', 'blonde', 'and', 'had', 'nearly', 'twice', 'the', 'usual', 'amount', 'of', 'neck', ',', 'which', 'came', 'in', 'very', 'useful', 'as', 'she', 'spent', 'so', 'much', 'of', 'her', 'time', 'c', '##rani', '##ng', 'over', 'garden', 'fences', ',', 'spying', 'on', 'the', 'neighbors', '.', 'little', 't', '##yke', ',', '\"', 'ch', '##ort', '##led', 'Mr', '.']\n"
     ]
    }
   ],
   "source": [
    "# Simple method getting tokens from text\n",
    "raw_text = '''Harry Potter and the Sorcerer's Stone (chapter 1)\n",
    "CHAPTER ONE\n",
    "\n",
    "THE BOY WHO LIVED\n",
    "\n",
    "Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much.\n",
    "They were the last people you'd expect to be involved in anything strange or mysterious, because they just didn't hold with such nonsense. \n",
    "Mr. Dursley was the director of a firm called Grunnings, which made drills. He was a big, beefy man with hardly any neck, although he did have a very large mustache.\n",
    "Mrs. Dursley was thin and blonde and had nearly twice the usual amount of neck, which came in very useful as she spent so much of her time craning over garden fences, spying on the neighbors. \n",
    "little tyke,\" chortled Mr.\n",
    "'''\n",
    "tokens = my_tokenizer.tokenize(raw_text)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'Harry', 'Potter', 'and', 'the', 'So', '##rcerer', \"'\", 's', 'Stone', '(', 'chapter', '1', ')', 'CHAPTER', 'ONE', 'THE', 'B', '##O', '##Y', 'WHO', 'L', '##IVE', '##D', 'Mr', '.', 'and', 'Mrs', '.', 'Du', '##rs', '##ley', ',', 'of', 'number', 'four', ',', 'P', '##rive', '##t', 'Drive', ',', 'were', 'proud', 'to', 'say', 'that', 'they', 'were', 'perfectly', 'normal', ',', 'thank', 'you', 'very', 'much', '.', 'They', 'were', 'the', 'last', 'people', 'you', \"'\", 'd', 'expect', 'to', 'be', 'involved', 'in', 'anything', 'strange', 'or', 'mysterious', ',', 'because', 'they', 'just', 'didn', \"'\", 't', 'hold', 'with', 'such', 'nonsense', '.', 'Mr', '.', 'Du', '##rs', '##ley', 'was', 'the', 'director', 'of', 'a', 'firm', 'called', 'G', '##run', '##ning', '##s', ',', 'which', 'made', 'drill', '##s', '.', 'He', 'was', 'a', 'big', ',', 'beef', '##y', 'man', 'with', 'hardly', 'any', 'neck', ',', 'although', 'he', 'did', 'have', 'a', 'very', 'large', 'must', '##ache', '.', 'Mrs', '.', 'Du', '##rs', '##ley', 'was', 'thin', 'and', 'blonde', 'and', 'had', 'nearly', 'twice', 'the', 'usual', 'amount', 'of', 'neck', ',', 'which', 'came', 'in', 'very', 'useful', 'as', 'she', 'spent', 'so', 'much', 'of', 'her', 'time', 'c', '##rani', '##ng', 'over', 'garden', 'fences', ',', 'spying', 'on', 'the', 'neighbors', '.', 'little', 't', '##yke', ',', '\"', 'ch', '##ort', '##led', 'Mr', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# This method also returns special tokens depending on the pretrained tokenizer\n",
    "detailed_tokens = my_tokenizer(raw_text).tokens()\n",
    "\n",
    "print(detailed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens: Integer ID Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 3466, 11434, 1105, 1103, 1573, 25989, 112, 188, 4118, 113, 6073, 122, 114, 8203, 24497, 7462, 139, 2346, 3663, 23750, 149, 26140, 2137, 1828, 119, 1105, 2823, 119, 12786, 1733, 1926, 117, 1104, 1295, 1300, 117, 153, 17389, 1204, 6877, 117, 1127, 6884, 1106, 1474, 1115, 1152, 1127, 6150, 2999, 117, 6243, 1128, 1304, 1277, 119, 1220, 1127, 1103, 1314, 1234, 1128, 112, 173, 5363, 1106, 1129, 2017, 1107, 1625, 4020, 1137, 8198, 117, 1272, 1152, 1198, 1238, 112, 189, 2080, 1114, 1216, 17466, 119, 1828, 119, 12786, 1733, 1926, 1108, 1103, 1900, 1104, 170, 3016, 1270, 144, 10607, 3381, 1116, 117, 1134, 1189, 15227, 1116, 119, 1124, 1108, 170, 1992, 117, 14413, 1183, 1299, 1114, 6374, 1251, 2455, 117, 1780, 1119, 1225, 1138, 170, 1304, 1415, 1538, 12804, 119, 2823, 119, 12786, 1733, 1926, 1108, 4240, 1105, 9853, 1105, 1125, 2212, 3059, 1103, 4400, 2971, 1104, 2455, 117, 1134, 1338, 1107, 1304, 5616, 1112, 1131, 2097, 1177, 1277, 1104, 1123, 1159, 172, 23851, 2118, 1166, 4605, 25617, 117, 26470, 1113, 1103, 11209, 119, 1376, 189, 27142, 117, 107, 22572, 12148, 2433, 1828, 119, 102]\n"
     ]
    }
   ],
   "source": [
    "# Way to get tokens as integer IDs\n",
    "print(my_tokenizer.encode(raw_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'Harry', 'Potter', 'and', 'the', 'So', '##rcerer', \"'\", 's', 'Stone', '(', 'chapter', '1', ')', 'CHAPTER', 'ONE', 'THE', 'B', '##O', '##Y', 'WHO', 'L', '##IVE', '##D', 'Mr', '.', 'and', 'Mrs', '.', 'Du', '##rs', '##ley', ',', 'of', 'number', 'four', ',', 'P', '##rive', '##t', 'Drive', ',', 'were', 'proud', 'to', 'say', 'that', 'they', 'were', 'perfectly', 'normal', ',', 'thank', 'you', 'very', 'much', '.', 'They', 'were', 'the', 'last', 'people', 'you', \"'\", 'd', 'expect', 'to', 'be', 'involved', 'in', 'anything', 'strange', 'or', 'mysterious', ',', 'because', 'they', 'just', 'didn', \"'\", 't', 'hold', 'with', 'such', 'nonsense', '.', 'Mr', '.', 'Du', '##rs', '##ley', 'was', 'the', 'director', 'of', 'a', 'firm', 'called', 'G', '##run', '##ning', '##s', ',', 'which', 'made', 'drill', '##s', '.', 'He', 'was', 'a', 'big', ',', 'beef', '##y', 'man', 'with', 'hardly', 'any', 'neck', ',', 'although', 'he', 'did', 'have', 'a', 'very', 'large', 'must', '##ache', '.', 'Mrs', '.', 'Du', '##rs', '##ley', 'was', 'thin', 'and', 'blonde', 'and', 'had', 'nearly', 'twice', 'the', 'usual', 'amount', 'of', 'neck', ',', 'which', 'came', 'in', 'very', 'useful', 'as', 'she', 'spent', 'so', 'much', 'of', 'her', 'time', 'c', '##rani', '##ng', 'over', 'garden', 'fences', ',', 'spying', 'on', 'the', 'neighbors', '.', 'little', 't', '##yke', ',', '\"', 'ch', '##ort', '##led', 'Mr', '.', '[SEP]']\n",
      "[101, 3466, 11434, 1105, 1103, 1573, 25989, 112, 188, 4118, 113, 6073, 122, 114, 8203, 24497, 7462, 139, 2346, 3663, 23750, 149, 26140, 2137, 1828, 119, 1105, 2823, 119, 12786, 1733, 1926, 117, 1104, 1295, 1300, 117, 153, 17389, 1204, 6877, 117, 1127, 6884, 1106, 1474, 1115, 1152, 1127, 6150, 2999, 117, 6243, 1128, 1304, 1277, 119, 1220, 1127, 1103, 1314, 1234, 1128, 112, 173, 5363, 1106, 1129, 2017, 1107, 1625, 4020, 1137, 8198, 117, 1272, 1152, 1198, 1238, 112, 189, 2080, 1114, 1216, 17466, 119, 1828, 119, 12786, 1733, 1926, 1108, 1103, 1900, 1104, 170, 3016, 1270, 144, 10607, 3381, 1116, 117, 1134, 1189, 15227, 1116, 119, 1124, 1108, 170, 1992, 117, 14413, 1183, 1299, 1114, 6374, 1251, 2455, 117, 1780, 1119, 1225, 1138, 170, 1304, 1415, 1538, 12804, 119, 2823, 119, 12786, 1733, 1926, 1108, 4240, 1105, 9853, 1105, 1125, 2212, 3059, 1103, 4400, 2971, 1104, 2455, 117, 1134, 1338, 1107, 1304, 5616, 1112, 1131, 2097, 1177, 1277, 1104, 1123, 1159, 172, 23851, 2118, 1166, 4605, 25617, 117, 26470, 1113, 1103, 11209, 119, 1376, 189, 27142, 117, 107, 22572, 12148, 2433, 1828, 119, 102]\n"
     ]
    }
   ],
   "source": [
    "print(detailed_tokens)\n",
    "\n",
    "# Tokenizer method to get the IDs if we already have the tokens as strings\n",
    "detailed_ids = my_tokenizer.convert_tokens_to_ids(detailed_tokens)\n",
    "print(detailed_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way can look a little complex but can be useful when working with\n",
    "tokenizers for certain tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 3466, 11434, 1105, 1103, 1573, 25989, 112, 188, 4118, 113, 6073, 122, 114, 8203, 24497, 7462, 139, 2346, 3663, 23750, 149, 26140, 2137, 1828, 119, 1105, 2823, 119, 12786, 1733, 1926, 117, 1104, 1295, 1300, 117, 153, 17389, 1204, 6877, 117, 1127, 6884, 1106, 1474, 1115, 1152, 1127, 6150, 2999, 117, 6243, 1128, 1304, 1277, 119, 1220, 1127, 1103, 1314, 1234, 1128, 112, 173, 5363, 1106, 1129, 2017, 1107, 1625, 4020, 1137, 8198, 117, 1272, 1152, 1198, 1238, 112, 189, 2080, 1114, 1216, 17466, 119, 1828, 119, 12786, 1733, 1926, 1108, 1103, 1900, 1104, 170, 3016, 1270, 144, 10607, 3381, 1116, 117, 1134, 1189, 15227, 1116, 119, 1124, 1108, 170, 1992, 117, 14413, 1183, 1299, 1114, 6374, 1251, 2455, 117, 1780, 1119, 1225, 1138, 170, 1304, 1415, 1538, 12804, 119, 2823, 119, 12786, 1733, 1926, 1108, 4240, 1105, 9853, 1105, 1125, 2212, 3059, 1103, 4400, 2971, 1104, 2455, 117, 1134, 1338, 1107, 1304, 5616, 1112, 1131, 2097, 1177, 1277, 1104, 1123, 1159, 172, 23851, 2118, 1166, 4605, 25617, 117, 26470, 1113, 1103, 11209, 119, 1376, 189, 27142, 117, 107, 22572, 12148, 2433, 1828, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns an object that has a few different keys available\n",
    "my_tokenizer(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 3466, 11434, 1105, 1103, 1573, 25989, 112, 188, 4118, 113, 6073, 122, 114, 8203, 24497, 7462, 139, 2346, 3663, 23750, 149, 26140, 2137, 1828, 119, 1105, 2823, 119, 12786, 1733, 1926, 117, 1104, 1295, 1300, 117, 153, 17389, 1204, 6877, 117, 1127, 6884, 1106, 1474, 1115, 1152, 1127, 6150, 2999, 117, 6243, 1128, 1304, 1277, 119, 1220, 1127, 1103, 1314, 1234, 1128, 112, 173, 5363, 1106, 1129, 2017, 1107, 1625, 4020, 1137, 8198, 117, 1272, 1152, 1198, 1238, 112, 189, 2080, 1114, 1216, 17466, 119, 1828, 119, 12786, 1733, 1926, 1108, 1103, 1900, 1104, 170, 3016, 1270, 144, 10607, 3381, 1116, 117, 1134, 1189, 15227, 1116, 119, 1124, 1108, 170, 1992, 117, 14413, 1183, 1299, 1114, 6374, 1251, 2455, 117, 1780, 1119, 1225, 1138, 170, 1304, 1415, 1538, 12804, 119, 2823, 119, 12786, 1733, 1926, 1108, 4240, 1105, 9853, 1105, 1125, 2212, 3059, 1103, 4400, 2971, 1104, 2455, 117, 1134, 1338, 1107, 1304, 5616, 1112, 1131, 2097, 1177, 1277, 1104, 1123, 1159, 172, 23851, 2118, 1166, 4605, 25617, 117, 26470, 1113, 1103, 11209, 119, 1376, 189, 27142, 117, 107, 22572, 12148, 2433, 1828, 119, 102]\n"
     ]
    }
   ],
   "source": [
    "# focus on `input_ids` which are the IDs associated with the tokens.\n",
    "print(my_tokenizer(raw_text).input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding: Tokens to Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We of course can use the tokenizer to go from token IDs to tokens and back to text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] Harry Potter and the Sorcerer\\'s Stone ( chapter 1 ) CHAPTER ONE THE BOY WHO LIVED Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you\\'d expect to be involved in anything strange or mysterious, because they just didn\\'t hold with such nonsense. Mr. Dursley was the director of a firm called Grunnings, which made drills. He was a big, beefy man with hardly any neck, although he did have a very large mustache. Mrs. Dursley was thin and blonde and had nearly twice the usual amount of neck, which came in very useful as she spent so much of her time craning over garden fences, spying on the neighbors. little tyke, \" chortled Mr. [SEP]'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Integer IDs for tokens\n",
    "ids = my_tokenizer.encode(raw_text)\n",
    "\n",
    "# The inverse of the .enocde() method: .decode()\n",
    "my_tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Harry Potter and the Sorcerer\\'s Stone ( chapter 1 ) CHAPTER ONE THE BOY WHO LIVED Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you\\'d expect to be involved in anything strange or mysterious, because they just didn\\'t hold with such nonsense. Mr. Dursley was the director of a firm called Grunnings, which made drills. He was a big, beefy man with hardly any neck, although he did have a very large mustache. Mrs. Dursley was thin and blonde and had nearly twice the usual amount of neck, which came in very useful as she spent so much of her time craning over garden fences, spying on the neighbors. little tyke, \" chortled Mr.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To ignore special tokens (depending on pretrained tokenizer)\n",
    "my_tokenizer.decode(ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'Harry',\n",
       " 'Potter',\n",
       " 'and',\n",
       " 'the',\n",
       " 'So',\n",
       " '##rcerer',\n",
       " \"'\",\n",
       " 's',\n",
       " 'Stone',\n",
       " '(',\n",
       " 'chapter',\n",
       " '1',\n",
       " ')',\n",
       " 'CHAPTER',\n",
       " 'ONE',\n",
       " 'THE',\n",
       " 'B',\n",
       " '##O',\n",
       " '##Y',\n",
       " 'WHO',\n",
       " 'L',\n",
       " '##IVE',\n",
       " '##D',\n",
       " 'Mr',\n",
       " '.',\n",
       " 'and',\n",
       " 'Mrs',\n",
       " '.',\n",
       " 'Du',\n",
       " '##rs',\n",
       " '##ley',\n",
       " ',',\n",
       " 'of',\n",
       " 'number',\n",
       " 'four',\n",
       " ',',\n",
       " 'P',\n",
       " '##rive',\n",
       " '##t',\n",
       " 'Drive',\n",
       " ',',\n",
       " 'were',\n",
       " 'proud',\n",
       " 'to',\n",
       " 'say',\n",
       " 'that',\n",
       " 'they',\n",
       " 'were',\n",
       " 'perfectly',\n",
       " 'normal',\n",
       " ',',\n",
       " 'thank',\n",
       " 'you',\n",
       " 'very',\n",
       " 'much',\n",
       " '.',\n",
       " 'They',\n",
       " 'were',\n",
       " 'the',\n",
       " 'last',\n",
       " 'people',\n",
       " 'you',\n",
       " \"'\",\n",
       " 'd',\n",
       " 'expect',\n",
       " 'to',\n",
       " 'be',\n",
       " 'involved',\n",
       " 'in',\n",
       " 'anything',\n",
       " 'strange',\n",
       " 'or',\n",
       " 'mysterious',\n",
       " ',',\n",
       " 'because',\n",
       " 'they',\n",
       " 'just',\n",
       " 'didn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'hold',\n",
       " 'with',\n",
       " 'such',\n",
       " 'nonsense',\n",
       " '.',\n",
       " 'Mr',\n",
       " '.',\n",
       " 'Du',\n",
       " '##rs',\n",
       " '##ley',\n",
       " 'was',\n",
       " 'the',\n",
       " 'director',\n",
       " 'of',\n",
       " 'a',\n",
       " 'firm',\n",
       " 'called',\n",
       " 'G',\n",
       " '##run',\n",
       " '##ning',\n",
       " '##s',\n",
       " ',',\n",
       " 'which',\n",
       " 'made',\n",
       " 'drill',\n",
       " '##s',\n",
       " '.',\n",
       " 'He',\n",
       " 'was',\n",
       " 'a',\n",
       " 'big',\n",
       " ',',\n",
       " 'beef',\n",
       " '##y',\n",
       " 'man',\n",
       " 'with',\n",
       " 'hardly',\n",
       " 'any',\n",
       " 'neck',\n",
       " ',',\n",
       " 'although',\n",
       " 'he',\n",
       " 'did',\n",
       " 'have',\n",
       " 'a',\n",
       " 'very',\n",
       " 'large',\n",
       " 'must',\n",
       " '##ache',\n",
       " '.',\n",
       " 'Mrs',\n",
       " '.',\n",
       " 'Du',\n",
       " '##rs',\n",
       " '##ley',\n",
       " 'was',\n",
       " 'thin',\n",
       " 'and',\n",
       " 'blonde',\n",
       " 'and',\n",
       " 'had',\n",
       " 'nearly',\n",
       " 'twice',\n",
       " 'the',\n",
       " 'usual',\n",
       " 'amount',\n",
       " 'of',\n",
       " 'neck',\n",
       " ',',\n",
       " 'which',\n",
       " 'came',\n",
       " 'in',\n",
       " 'very',\n",
       " 'useful',\n",
       " 'as',\n",
       " 'she',\n",
       " 'spent',\n",
       " 'so',\n",
       " 'much',\n",
       " 'of',\n",
       " 'her',\n",
       " 'time',\n",
       " 'c',\n",
       " '##rani',\n",
       " '##ng',\n",
       " 'over',\n",
       " 'garden',\n",
       " 'fences',\n",
       " ',',\n",
       " 'spying',\n",
       " 'on',\n",
       " 'the',\n",
       " 'neighbors',\n",
       " '.',\n",
       " 'little',\n",
       " 't',\n",
       " '##yke',\n",
       " ',',\n",
       " '\"',\n",
       " 'ch',\n",
       " '##ort',\n",
       " '##led',\n",
       " 'Mr',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of tokens as strings instead of one long string\n",
    "my_tokenizer.convert_ids_to_tokens(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Note on the Unknown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> One thing to consider is if a string is outside of the tokenizer's vocabulary,\n",
    "> also known as an \"unkown\" token.\n",
    "> \n",
    "> They are typically represented with `[UNK]` or\n",
    "> some other similar variant.\n",
    "\n",
    "\n",
    "<!--\n",
    "If the tokenizer encoded the text so each character was a token (which is\n",
    "actually not as easy as it sounds), then it would be impossible to have an\n",
    "\"unknown\" token. Word-based tokenization will always be in danger of having \n",
    "\"unknown\" tokens since it's virtually impossible to have every possible word (\n",
    "and \"non-word\") in its vocabulary!\n",
    "\n",
    "And so you might think that subword tokenization wouldn't have an issue with\n",
    "\"unknown\" tokens. And although there are fewer than word-based tokenization, it\n",
    "does happen!\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "Tokenizers are specific so it's important to use a tokenizer that will recognize\n",
    "most of the text you're working with! For example, a lot of tokenizers might not\n",
    "consider emoji as tokens but could be really important if emoji are especially\n",
    "numerous in your data (like a corpus of chat messages)!\n",
    "\n",
    "If you're seeing a lot of \"unknown\" tokens with the text you're working with,\n",
    "might consider using a different tokenizer appropiate for the task. Or it's also\n",
    "possible to fine-tune a pretrained model or train one from scratch!\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🥱 the dog next door kept barking all night!!\n",
      "['[CLS]', '[UNK]', 'the', 'dog', 'next', 'door', 'kept', 'barking', 'all', 'night', '!', '!', '[SEP]']\n",
      "[CLS] [UNK] the dog next door kept barking all night!! [SEP]\n"
     ]
    }
   ],
   "source": [
    "phrase = '🥱 the dog next door kept barking all night!!'\n",
    "ids = my_tokenizer.encode(phrase)\n",
    "print(phrase)\n",
    "print(my_tokenizer.convert_ids_to_tokens(ids))\n",
    "print(my_tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wow my dad thought mcdonalds sold tacos 💀\n",
      "['[CLS]', 'w', '##ow', 'my', 'dad', 'thought', 'm', '##c', '##don', '##ald', '##s', 'sold', 'ta', '##cos', '[UNK]', '[SEP]']\n",
      "[CLS] wow my dad thought mcdonalds sold tacos [UNK] [SEP]\n"
     ]
    }
   ],
   "source": [
    "phrase = '''wow my dad thought mcdonalds sold tacos \\N{SKULL}'''\n",
    "ids = my_tokenizer.encode(phrase)\n",
    "print(phrase)\n",
    "print(my_tokenizer.convert_ids_to_tokens(ids))\n",
    "print(my_tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
